# -*- coding: utf-8 -*-
"""SCP-Chatbot JSON Process+Retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lCK6Y4V0uaFlXRv1uV1ZSav8ND9JVPTD
"""

!pip install -qU langchain langchain-community bs4 pinecone-client pinecone-text langchain-openai langchain-pinecone pinecone-notebooks jq

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()
os.environ["PINECONE_API_KEY"] = getpass.getpass()
os.environ["PINECONE_ENVIRONMENT"] = getpass.getpass()
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()

# import dotenv

# dotenv.load_dotenv()

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
import bs4
import time
from tqdm.auto import tqdm
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import JSONLoader
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.retrievers import PineconeHybridSearchRetriever
from pinecone_text.sparse import BM25Encoder
from langchain_core.messages import AIMessage, HumanMessage
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnablePassthrough

json_directory = './data/'  # Directory containing the JSON files

# Function to extract metadata from the JSON record
def metadata_func(record: dict, metadata: dict) -> dict:
    for key, value in record.items():
        if key != "content":
            metadata[key] = value
    return metadata

# Function to process a single JSON file
def process_json_file(file_path):
    loader = JSONLoader(
        file_path=file_path,
        jq_schema='.',
        content_key="content",
        metadata_func=metadata_func
    )

    data = loader.load()

    print(f"Processing file: {file_path}")  # Debug: Print the file being processed
    for doc in data:
        print(f"Content: {doc.page_content[:100]}...")  # Debug: Print the first 100 characters of the content
        print("-----------")
        print(f"Metadata: {doc.metadata}")
        print("===========")  # Debug: Separator for each document

    return data

# Initialize the text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0,
    length_function=len,
    is_separator_regex=False
)

# Get a list of JSON files in the specified directory
json_files = [file for file in os.listdir(json_directory) if file.endswith('.json')]

# Initialize an empty list to store the document chunks
splits = []

# Iterate over each JSON file
for json_file in json_files:
    # Construct the file path
    file_path = os.path.join(json_directory, json_file)

    # Process the JSON file
    data = process_json_file(file_path)

    # Split the documents into chunks and extend the splits list
    splits.extend(text_splitter.split_documents(data))

# Print the total number of chunks
print(f"Total number of chunks: {len(splits)}")
print(f"First 5 chunks: {splits[:5]}")
print(f"Last 5 chunks: {splits[-5:]}")

# Instantiate the OpenAI embeddings model
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

import tiktoken

# Function to count tokens in a string
def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

# Calculate token counts for each chunk
encoding_name = "cl100k_base"  # Appropriate encoding for the embedding model
token_counts = [num_tokens_from_string(chunk.page_content, encoding_name) for chunk in splits]

# Calculate total tokens and average tokens per chunk
total_tokens = sum(token_counts)
average_tokens = total_tokens / len(token_counts)

# Find the indices of the chunks with min and max tokens
min_tokens = min(token_counts)
max_tokens = max(token_counts)
min_index = token_counts.index(min_tokens)
max_index = token_counts.index(max_tokens)

# Calculate the cost of embeddings
cost_per_1000_tokens = 0.00013  # Actual cost per 1,000 tokens in USD for "text-embedding-3-large"
total_cost = (total_tokens / 1000) * cost_per_1000_tokens

# Print summary statistics and estimated cost
print(f"Total tokens: {total_tokens}")
print(f"Average tokens per chunk: {average_tokens}")
print(f"Max tokens in a chunk: {max_tokens} (Chunk number: {max_index + 1})")
print(f"Min tokens in a chunk: {min_tokens} (Chunk number: {min_index + 1})")
print(f"Estimated cost of embeddings: ${total_cost:.4f}")

pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"], environment=os.environ["PINECONE_ENVIRONMENT"])

index_name = "scp-data"

# check if index exists
print(f"Index stats: {pc.describe_index(index_name)}")

if index_name in pc.list_indexes().names():
    pc.delete_index(index_name)
    print(f"Deleted index: {index_name}")
# create a new index
pc.create_index(
    index_name,
    dimension=1536,
    metric='dotproduct',
    spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1",
        )
)
print(f"Created new index: {index_name}")

# wait for index to be initialized
while not pc.describe_index(index_name).status['ready']:
    print(f"Waiting for index {index_name} to be initialized...")
    time.sleep(1)

index = pc.Index(index_name)
index_stats = index.describe_index_stats()
print(f"New index stats: {index_stats}")  # Debug: Print the index stats

batch_size = 100  # Adjust the batch size as needed
print(f"Total number of chunks: {len(splits)}")  # Debugging: Print the total number of chunks

for i in range(0, len(splits), batch_size):
    print(f"Processing batch {i // batch_size + 1}")  # Debugging: Print the current batch number

    # Get a batch of documents from the splits list
    batch = splits[i:i+batch_size]
    print(f"Number of documents in the batch: {len(batch)}")  # Debugging: Print the number of documents in the batch

    # Convert documents to dictionaries
    vectors = []
    for doc in batch:
        # Generate a unique ID for each document using the hash of its content
        doc_id = str(hash(doc.page_content))
        print(f"Processing document with ID: {doc_id}")  # Debugging: Print the ID of the document being processed

        # Embed the document content using the embeddings model
        doc_embedding = embeddings.embed_query(doc.page_content)
        print(f"Embedding length: {len(doc_embedding)}")  # Debugging: Print the length of the document embedding

        # Create a vector dictionary with the document ID, embedding, and metadata
        vector = {
            "id": doc_id,
            "values": doc_embedding,
            "metadata": {
                "context": doc.page_content,  # Include the document content in the metadata
                **doc.metadata  # Include the existing metadata
            }
        }
        vectors.append(vector)

    print(f"Upserting {len(vectors)} vectors to the index")  # Debugging: Print the number of vectors being upserted

    # Upsert the vectors to the Pinecone index
    index.upsert(vectors)

    print(f"Batch {i // batch_size + 1} processed successfully")  # Debugging: Print a success message for the batch

print(f"Index stats: {index_stats}")

print(f"Splits: {splits[0]}")
print(f"Length of splits: {len(splits)} chunks")

# Instantiate the OpenAI chat model
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0.5)
llm.invoke("what is scp-001?")

text_field = "context"
vectorstore = PineconeVectorStore.from_existing_index(index_name, embeddings, text_field)
retriever = vectorstore.as_retriever()
#print(f"Vectorstore: {vectorstore}")
#print(f"Vectorstore type: {type(vectorstore)}")
#print(f"Vectorstore.embeddings: {vectorstore.embeddings}")
#print(f"Retriever: {retriever}")
#print(f"Retriever type: {type(retriever)}")

docs = retriever.invoke("what is scp-003?")
#print(f"Docs: {docs}")
#print(f"Docs type: {type(docs)}")

query = "which scp is described ass tumorous growth?"
vectorstore.similarity_search(
    query,  # our search query
    k=3  # return 3 most relevant docs
)

### Contextualize question ###
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""


contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

### Answer question ###
qa_system_prompt = """You are a snarky and very rude assistant for question-answering SCP related tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Always add a snarky comment to your response and occasionally a tasteless pun.\

{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

### Statefully manage chat history ###
store = {}


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def trim_messages(session_id, max_messages, chain_input):
    stored_messages = get_session_history(session_id).messages
    if len(stored_messages) > max_messages:
        get_session_history(session_id).clear()
        for message in stored_messages[-max_messages:]:
            get_session_history(session_id).add_message(message)
    return chain_input


max_messages = 5  # Specify the maximum number of messages to keep in the history


conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

conversational_rag_chain_with_trimming = RunnablePassthrough.assign(
    chain_input=lambda chain_input, config: trim_messages(config["configurable"]["session_id"], max_messages, chain_input)
) | conversational_rag_chain

# Test conversation
test_conversation = [
    ("Who is general Mulhausen?", "General Mulhausen is a character in the SCP Foundation universe."),
    ("What is his role?", "General Mulhausen is a high-ranking military officer involved with the SCP Foundation."),
    ("Did he die?", "According to the SCP lore, General Mulhausen did die at some point."),
    ("How did he die?", "The exact circumstances of General Mulhausen's death are not clearly specified in the SCP stories."),
    ("What SCP was involved?", "I'm not sure which specific SCP was directly involved in General Mulhausen's death."),
    ("What are some theories?", "There are various theories and speculations among SCP fans about how General Mulhausen died, but no definitive answer."),
]

# Initialize an empty chat history for testing
test_session_id = "test_session"
store[test_session_id] = ChatMessageHistory()

# Simulate the conversation
for user_input, _ in test_conversation:
    result = conversational_rag_chain_with_trimming.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": test_session_id}},
    )["answer"]
    print(f"User: {user_input}")
    print(f"Assistant: {result}\n")

    # Print the stored messages after each turn
    print("Stored Messages:")
    for message in get_session_history(test_session_id).messages:
        print(f"- {message}")
    print()

conversational_rag_chain.invoke(
    {"input": "Who is general Mulhausen?"},
    config={
        "configurable": {"session_id": "abc123"}
    },  # constructs a key "abc123" in `store`.
)["answer"]

conversational_rag_chain.invoke(
    {"input": "Did he die?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain.invoke(
    {"input": "Who or what killed him?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain_with_trimming.invoke(
    {"input": "remember this word: lovecraft"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain_with_trimming.invoke(
    {"input": "Who is general Mulhausen?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain_with_trimming.invoke(
    {"input": "Whats the word I wanted you to remember?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain_with_trimming.invoke(
    {"input": "Did he die?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

conversational_rag_chain_with_trimming.invoke(
    {"input": "Who or what killed him?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]

"""## Pinecone Hybrid Search Retriever"""

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langchain_community.retrievers import (
    PineconeHybridSearchRetriever,
)
from pinecone_text.sparse import BM25Encoder

# Initialize the BM25 encoder
bm25_encoder = BM25Encoder().default()

# Fit the BM25 encoder on the text chunks
bm25_encoder.fit([chunk.page_content for chunk in splits])

# Initialize the PineconeHybridSearchRetriever
retriever = PineconeHybridSearchRetriever(
    embeddings=embeddings,
    sparse_encoder=bm25_encoder,
    index=index_name
)

# Test the retriever
query = "What is the containment procedure for SCP-999?"
print(f"Query: {query}\n")
results = retriever.get_relevant_documents(query)
print(f"Results: {results}\n")

# Print the retrieved documents
for doc in results:
    print(f"Content: {doc.page_content}\n")
    print(f"Metadata: {doc.metadata}\n")
    print("---")